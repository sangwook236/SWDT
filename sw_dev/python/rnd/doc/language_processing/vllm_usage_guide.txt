[-] General.
	- Site.
		https://github.com/vllm-project
		https://github.com/vllm-project/vllm

		https://docs.vllm.ai/en/latest/

		https://docs.vllm.ai/en/latest/models/supported_models.html

[-] Usage.
	https://docs.vllm.ai/en/latest/getting_started/quickstart.html

	vllm --help
	vllm serve --help

	- Install.
		Using uv:
			uv venv --python 3.12 --seed
			source .venv/bin/activate
			uv pip install vllm --torch-backend=auto

			uv run --with vllm vllm --help

		Using conda:
			conda create -n myenv python=3.12 -y
			conda activate myenv
			pip install --upgrade uv
			uv pip install vllm --torch-backend=auto

	- Offline inference.
		https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/basic/basic.py

	- Online serving.
		OpenAI-compatible server:
			vLLM can be deployed as a server that implements the OpenAI API protocol.
			This allows vLLM to be used as a drop-in replacement for applications using OpenAI API.
			By default, it starts the server at http://localhost:8000.
			You can specify the address with --host and --port arguments.
				vllm serve --model <MODE_NAME> --host 0.0.0.0 --port 8000  # For remote access
			The server currently hosts one model at a time and implements endpoints such as list models, create chat completion, and create completion endpoints.

			Run the following command to start the vLLM server with the Qwen2.5-1.5B-Instruct model:
				vllm serve Qwen/Qwen2.5-1.5B-Instruct
			This server can be queried in the same format as OpenAI API. For example, to list the models:
				curl http://localhost:8000/v1/models
			You can pass in the argument --api-key or environment variable VLLM_API_KEY to enable the server to check for API key in the header.

[-] Distributed inference and serving.
	https://docs.vllm.ai/en/latest/serving/distributed_serving.html

	- Run vLLM on a single node.
		To run multi-GPU inference with the LLM class, set the tensor_parallel_size argument to the number of GPUs you want to use.
		For example, to run inference on 4 GPUs:
			from vllm import LLM
			llm = LLM("facebook/opt-13b", tensor_parallel_size=4)
			output = llm.generate("San Francisco is a")

		To run multi-GPU serving, pass in the --tensor-parallel-size argument when starting the server.
		For example, to run API server on 4 GPUs:
			vllm serve facebook/opt-13b --tensor-parallel-size 4
		You can also additionally specify --pipeline-parallel-size to enable pipeline parallelism.
		For example, to run API server on 8 GPUs with pipeline parallelism and tensor parallelism:
			vllm serve gpt2 --tensor-parallel-size 4 --pipeline-parallel-size 2

	- Run vLLM on multiple nodes.
		Pick a node as the head node, and run the following command:
			bash run_cluster.sh \
				vllm/vllm-openai \
				ip_of_head_node \
				--head \
				/path/to/the/huggingface/home/in/this/node \
				-e VLLM_HOST_IP=ip_of_this_node
		On the rest of the worker nodes, run the following command:
			bash run_cluster.sh \
				vllm/vllm-openai \
				ip_of_head_node \
				--worker \
				/path/to/the/huggingface/home/in/this/node \
				-e VLLM_HOST_IP=ip_of_this_node

[-] Installation.
	https://docs.vllm.ai/en/latest/getting_started/installation/

	- Install.
		https://docs.vllm.ai/en/latest/getting_started/installation/index.html

		Using pip:
			pip install vllm --extra-index-url https://download.pytorch.org/whl/cu128
		Using uv:
			uv pip install vllm --torch-backend=auto

		python -m vllm.entrypoints.openai.api_server \
			--model Qwen/Qwen3-4B \
			--host 0.0.0.0 --port 8000

	- Docker.
		https://docs.vllm.ai/en/stable/deployment/docker.html

		docker pull vllm/vllm-openai:latest

		docker run --runtime nvidia --gpus all \
			-v ~/.cache/huggingface:/root/.cache/huggingface \
			--env "HUGGING_FACE_HUB_TOKEN=$HF_TOKEN" \
			-p 8000:8000 \
			--ipc=host \
			vllm/vllm-openai:latest \
			--model Qwen/Qwen3-0.6B

	- Check.
		curl http://localhost:8000/v1/models
		curl http://localhost:8000/v1/metrics
