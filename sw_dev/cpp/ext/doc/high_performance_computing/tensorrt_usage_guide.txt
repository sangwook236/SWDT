[-] General.
	- Site.
		https://developer.nvidia.com/tensorrt

		https://github.com/NVIDIA/TensorRT
		https://github.com/NVIDIA/TRTorch
		https://pytorch.org/TensorRT/
		https://github.com/pytorch/TensorRT

		https://docs.nvidia.com/deeplearning/tensorrt
		https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html
		https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html
		https://nvidia.github.io/TRTorch/

[-] Usage (Python).
	- TenorRT.
		TensorRT-7.2.1.6/samples/python
			Download from https://developer.nvidia.com/tensorrt
			tar xzvf TensorRT-7.2.1.6.Ubuntu-18.04.x86_64-gnu.cuda-11.0.cudnn8.0.tar.gz

		https://si-analytics.tistory.com/33 (A few bugs)
			${SWDT_PYTHON_HOME}/ext/test/high_performance_computing/tensorrt_test.py

	- TRTorch.
		https://github.com/NVIDIA/TRTorch/blob/master/notebooks/Resnet50-example.ipynb
			${SWDT_PYTHON_HOME}/ext/test/high_performance_computing/tensorrt_test.py

	- Torch-TensorRT.
		https://github.com/pytorch/TensorRT/tree/master/examples/fx
			${SWDT_PYTHON_HOME}/rnd/test/machine_learning/pytorch/pytorch_tensorrt.py

	- ONNX to TensorRT.
		https://github.com/open-mmlab/mmdetection/blob/master/docs/en/tutorials/onnx2tensorrt.md
		https://github.com/tier4/sam2_trt_inference

[-] Tools.
	- trtexec.
		https://docs.nvidia.com/deeplearning/tensorrt/latest/reference/command-line-programs.html

		trtexec --onnx=path/to/model.onnx --saveEngine=path/to/model.engine --fp16 --workspace=4096 --shapes=input:1024x1024x3

[-] Installation (TensorRT).
	- Install CUDA and cuDNN.
		Using docker:
			https://hub.docker.com/

			sudo docker pull nvidia/cuda:11.0-cudnn8-devel-ubuntu18.04
			sudo docker pull nvidia/cuda

			If you have Docker 19.03 or later:
				docker run --gpus all -it --rm -v local_dir:container_dir nvidia/cuda:11.0-cudnn8-devel-ubuntu18.04
			If you have Docker 19.02 or earlier:
				nvidia-docker run -it --rm -v local_dir:container_dir nvidia/cuda:11.0-cudnn8-devel-ubuntu18.04

				sudo docker run --gpus all -it --rm -v ~:/workspace/sangwook nvidia/cuda:11.0-cudnn8-devel-ubuntu18.04
			apt -y update && apt -y upgrade
	- Install CUDA, cuDNN, and TensorRT.
		Using docker:
			NVIDIA NGC:
				https://ngc.nvidia.com/catalog/containers/nvidia:tensorrt

			sudo docker pull nvcr.io/nvidia/tensorrt:20.08-py3

			If you have Docker 19.03 or later:
				docker run --gpus all -it --rm -v local_dir:container_dir nvcr.io/nvidia/tensorrt:<xx.xx>-py<x>
			If you have Docker 19.02 or earlier:
				nvidia-docker run -it --rm -v local_dir:container_dir nvcr.io/nvidia/tensorrt:<xx.xx>-py<x>

				sudo docker run --gpus all -it --rm -v ~:/workspace/sangwook nvcr.io/nvidia/tensorrt:20.08-py3
			apt -y update && apt -y upgrade

			(Optional) Build samples:
				cd /workspace/tensorrt/samples
				make -j4
				cd /workspace/tensorrt/bin
				./sample_mnist
					<error>
					Cudnn Error in initializeCommonContext: 4 (Could not initialize cudnn, please check cudnn installation.)

	- Install TensorRT.
		https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html
		https://developer.nvidia.com/tensorrt

		From source:
			https://github.com/NVIDIA/TensorRT

			Download TensorRT OSS sources:
				git clone https://github.com/NVIDIA/TensorRT.git
				cd ${TensorRT_HOME}
				git submodule update --init --recursive

			Download the TensorRT binary release:
				Download from https://developer.nvidia.com/tensorrt
				cd /path/to/downloads
				tar -xvzf TensorRT-7.2.1.6.Ubuntu-18.04.x86_64-gnu.cuda-11.0.cudnn8.0.tar.gz

			Download CMake:
				wget https://github.com/Kitware/CMake/releases/download/v3.18.4/cmake-3.18.4-Linux-x86_64.tar.gz
				tar zxvf cmake-3.18.4-Linux-x86_64.tar.gz

			Run a docker image:
				From NVIDIA docker image:
					docker run --gpus all -it --rm -v local_dir:container_dir nvidia/cuda:11.0-cudnn8-devel-ubuntu18.04
					
					sudo docker run --gpus all -it --rm -v ~:/workspace/sangwook nvidia/cuda:11.0-cudnn8-devel-ubuntu18.04

					(Optional) Install cuDNN.
						https://developer.nvidia.com/cudnn
					Install libraries:
						sudo apt -y install pkg-config libpthread-stubs0-dev zlib1g-dev cmake
					ln -s /usr/bin/python3 /usr/bin/python

				From TensorRT OSS sources:
					cd ${TRT_SOURCE}
					Build a docker image.
						./docker/build.sh --file docker/ubuntu.Dockerfile --tag tensorrt-ubuntu --os 18.04 --cuda 11.0
							ID: trtuser, PW: nvidia.

					Launch the TensorRT build container.
						./docker/launch.sh --tag tensorrt-ubuntu --gpus all --release $TRT_RELEASE --source $TRT_SOURCE

						docker run --gpus all -it --rm -v ~:/workspace/sangwook --shm-size 16G --name my_tensorrt tensorrt-ubuntu:latest

			Configure environment:
				export LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/lib:$LD_LIBRARY_PATH
				export PATH=${CMAKE_HOME}/bin:$PATH				

				apt -y install libglu1-mesa-dev freeglut3-dev mesa-common-dev

				(Optional) apt -y remove python3.6 && apt -y autoremove
				apt -y install python3.7-dev
					Needs Python 3.7 or later.
				rm /usr/bin/python /usr/bin/python3
				ln -s /usr/bin/python3.7 /usr/bin/python
				ln -s /usr/bin/python3.7 /usr/bin/python3
				apt -y install python3-pip python3-setuptools
				python -m pip install --upgrade pip

			Build the TensorRT OSS components.
				export TRT_RELEASE=/path/to/TensorRT-7.2.1.6
				export TRT_SOURCE=/path/to/TensorRT_github
				export PATH=~/.local/bin:$PATH

				cd ${TRT_SOURCE}
				mkdir -p build && cd build
				cmake .. -DTRT_LIB_DIR=$TRT_RELEASE/lib -DTRT_OUT_DIR=`pwd`/out
					cmake .. -DTRT_LIB_DIR=$TRT_RELEASE/lib -DTRT_OUT_DIR=`pwd`/out -DCUB_ROOT_DIR=
				make -j$(nproc)

				sudo make install
					${TRT_RELEASE}/targets/x86_64-linux-gnu/include
					${TRT_RELEASE}/targets/x86_64-linux-gnu/lib
					${TRT_RELEASE}/targets/x86_64-linux-gnu/bin

		Using deb:
			https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html

			Download the TensorRT local repo file.
				https://developer.nvidia.com/tensorrt
				https://developer.nvidia.com/nvidia-tensorrt-download

			Install TensorRT from the Debian local repo package.
				os="ubuntuxx04"
				tag="cudax.x-trt8.x.x.x-ga-yyyymmdd"
				sudo dpkg -i nv-tensorrt-repo-${os}-${tag}_amd64.deb
					All deb files are in /var/nv-tensorrt-repo-${os}-${tag}.

					sudo dpkg -i nv-tensorrt-repo-ubuntu2004-cuda11.6-trt8.4.3.1-ga-20220813_1-1_amd64.deb
					ls /var/nv-tensorrt-repo-ubuntu2004-cuda11.6-trt8.4.3.1-ga-20220813/
				sudo apt-key add /var/nv-tensorrt-repo-${os}-${tag}/*.pub
					sudo apt-key add /var/nv-tensorrt-repo-ubuntu2004-cuda11.6-trt8.4.3.1-ga-20220813/c1c4ee19.pub
					sudo apt-key add /var/nv-tensorrt-repo-ubuntu2004-cuda11.6-trt8.4.3.1-ga-20220813/*.pub

				sudo apt update
				sudo apt install tensorrt
					sudo dpkg -i /var/nv-tensorrt-repo-ubuntu2004-cuda11.6-trt8.4.3.1-ga-20220813/tensorrt_8.4.3.1-1+cuda11.6_amd64.deb

				If using Python 3.x:
					pip install numpy
					sudo apt install python3-libnvinfer-dev

				If you plan to use TensorRT with TensorFlow:
					pip install protobuf
					sudo apt install uff-converter-tf

				If you would like to run the samples that require ONNX graphsurgeon or use the Python module for your own project:
					pip install numpy onnx
					sudo apt install onnx-graphsurgeon

				Verify the installation.
					dpkg -l | grep TensorRT

		Using apt:
			For only running TensorRT C++ applications:
				apt -y install libnvinfer7 libnvonnxparsers7 libnvparsers7 libnvinfer-plugin7
			For also building TensorRT C++ applications:
				apt -y install libnvinfer-dev libnvonnxparsers-dev libnvparsers-dev libnvinfer-plugin-dev
				apt -y install libnvinfer-dev=8.4.3-1+cuda11.6 libnvinfer-plugin-dev=8.4.3-1+cuda11.6
			For running TensorRT Python applications:
				apt -y install python-libnvinfer python3-libnvinfer

			Install libnvinfer7 for an older CUDA version and hold the libnvinfer7 package at this version:
				version="7.2.0-1+cuda11.0"
				apt -y install libnvinfer7=${version} libnvonnxparsers7=${version} libnvparsers7=${version} libnvinfer-plugin7=${version} libnvinfer-dev=${version} libnvonnxparsers-dev=${version} libnvparsers-dev=${version} libnvinfer-plugin-dev=${version} python-libnvinfer=${version} python3-libnvinfer=${version}
				apt-mark hold libnvinfer7 libnvonnxparsers7 libnvparsers7 libnvinfer-plugin7 libnvinfer-dev libnvonnxparsers-dev libnvparsers-dev libnvinfer-plugin-dev python-libnvinfer python3-libnvinfer
			Upgrade to the latest version of TensorRT or the latest version of CUDA:
				apt-mark unhold libnvinfer7 libnvonnxparsers7 libnvparsers7 libnvinfer-plugin7 libnvinfer-dev libnvonnxparsers-dev libnvparsers-dev libnvinfer-plugin-dev python-libnvinfer python3-libnvinfer

		Check TensorRT.
			dpkg -l | grep nvinfer

	- Install TensorRT Python.
		pip install pycuda torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html
		pip install pycuda torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html

		pip install opencv-python onnx transformers

		Using tar package:
			https://eehoeskrap.tistory.com/302

			Download from https://developer.nvidia.com/tensorrt

			tar xzvf TensorRT-7.2.1.6.Ubuntu-18.04.x86_64-gnu.cuda-11.0.cudnn8.0.tar.gz
			export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/path/to/TensorRT-7.2.1.6/lib

			cd TensorRT-7.2.1.6/python
			pip install tensorrt-7.2.1.6-cp36-none-linux_x86_64.whl
			pip install tensorrt-7.2.1.6-cp37-none-linux_x86_64.whl

			cd TensorRT-7.2.1.6/uff
			pip install uff-0.6.9-py2.py3-none-any.whl
			which convert-to-uff

			cd TensorRT-7.2.1.6/graphsurgeon
			pip install graphsurgeon-0.4.5-py2.py3-none-any.whl

		Check TensorRT.
			tree-d
			python -c "import tensorrt"

	- Build a docker image.
		https://si-analytics.tistory.com/32

		Edit a Dockerfile.
			./Dockerfile.cuda & ./Dockerfile.tensorrt
			Recommend using Dockerfile.cuda.
				Dockerfile.tensorrt is smaller but not good.

		Rename Dockerfile.cuda to Dockerfile.
		docker build --tag sangwook/cuda:latest .
			This command "docker build --tag sangwook/cuda:latest - < Dockerfile.cuda" causes an error.

		Rename Dockerfile.tensorrt to Dockerfile.
		docker build --tag sangwook/tensorrt:latest .
			This command "docker build --tag sangwook/tensorrt:latest - < Dockerfile.tensorrt" causes an error.

	- Run a docker image.
		docker run --gpus all -it --rm -v ~:/workspace/sangwook --shm-size 16G --name my_tensorrt sangwook/tensorrt:latest python3 -c "import tensorrt"
		NV_GPU=0 nvidia-docker run -it --rm -v ~:/workspace/sangwook --shm-size 16G --name my_tensorrt sangwook/tensorrt:latest python3 -c "import tensorrt"

		docker run --gpus all -it --rm -v ~:/workspace/sangwook --shm-size 16G --name my_cuda sangwook/cuda:latest python3 -c "import tensorrt"
		NV_GPU=0 nvidia-docker run -it --rm -v ~:/workspace/sangwook --shm-size 16G --name my_cuda sangwook/cuda:latest python3 -c "import tensorrt"

[-] Installation (TRTorch).
	- Install bazel (For building TRTorch from source).
		https://github.com/bazelbuild/bazel/releases

		Using apt:
			https://docs.bazel.build/versions/master/install-ubuntu.html

			apt install curl gnupg
			curl -fsSL https://bazel.build/bazel-release.pub.gpg | gpg --dearmor > bazel.gpg
			mv bazel.gpg /etc/apt/trusted.gpg.d/
			echo "deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8" | tee /etc/apt/sources.list.d/bazel.list

			apt update && apt install bazel
			apt update && apt full-upgrade
			apt install bazel-3.4.1

			ln -s /usr/bin/bazel-3.4.1 /usr/bin/bazel

		From source:
			https://github.com/NVIDIA/TRTorch

			Install JDK:
				apt install openjdk-11-jdk
				apt install default-jdk
			apt install zip

			export BAZEL_VERSION=3.4.1
			mkdir bazel
			cd bazel
			curl -fSsL -O https://github.com/bazelbuild/bazel/releases/download/$BAZEL_VERSION/bazel-$BAZEL_VERSION-dist.zip
			unzip bazel-$BAZEL_VERSION-dist.zip
			bash ./compile.sh

	- Install TRTorch.
		https://github.com/NVIDIA/TRTorch
		https://nvidia.github.io/TRTorch/tutorials/installation.html

		Dependency:
			Bazel 3.3.1
			Libtorch 1.5.1
			CUDA 10.2
			cuDNN 7.6.5 (by default, cuDNN 8 supported with compatable PyTorch build)
			TensorRT 7.0.0 (by default, TensorRT 7.1 supported with compatable PyTorch build)
	
		From source:
			git clone https://github.com/NVIDIA/TRTorch.git
			cd ${TRTorch_HOME}

			export TRT_RELEASE=/path/to/TensorRT-7.2.1.6
			mkdir ${TRT_RELEASE}/include/x86_64-linux-gnu ${TRT_RELEASE}/lib/x86_64-linux-gnu
			cp ${TRT_RELEASE}/include/*.h ${TRT_RELEASE}/include/x86_64-linux-gnu
			cp ${TRT_RELEASE}/lib/* ${TRT_RELEASE}/lib/x86_64-linux-gnu

			Edit ${TRTorch_HOME}/WORKSPCE.
				https://github.com/NVIDIA/TRTorch

				new_local_repository(
					name = "cudnn",
					path = "/usr/",
					build_file = "@//third_party/cudnn/local:BUILD"
				)

				new_local_repository(
				   name = "tensorrt",
				#   path = "/usr/",
				   path = "${TRT_RELEASE}",
				   build_file = "@//third_party/tensorrt/local:BUILD"
				)

			ln -s /usr/local/cuda-11.0 /usr/local/cuda-10.2
				CUDA 10.2 is used.
			(Optional) ln -s /usr/lib/x86_64-linux-gnu/libnvinfer.so /usr/lib/x86_64-linux-gnu/libnvinfer_static.so
				nvinfer_static is linked to build libtrtorch.

			Edit ${TRTorch_HOME}/cpp/trtorchc/BUILD
				cc_binary(
					name = "trtorchc",
					srcs = [
						"main.cpp"
					],
					deps = [
						"//third_party/args",
						"//cpp/api:trtorch"
					] + select({
						":use_pre_cxx11_abi":  [
							"@libtorch_pre_cxx11_abi//:libtorch",
							"@libtorch_pre_cxx11_abi//:caffe2",
						],
						"//conditions:default":  [
							"@libtorch//:libtorch",
							"@libtorch//:caffe2",
						],
					}),
					linkstatic = False,  # Add.
				)
			Edit ${TRTorch_HOME}/cpp/api/lib/BUILD
				cc_binary(
					name = "libtrtorch.so",
					srcs = [],
					deps = [
						"//cpp/api:trtorch"
					],
					linkstatic = False,  # Change.
					linkshared = True
				)

				cc_binary(
					name = "trtorch.dll",
					srcs = [],
					deps = [
						"//cpp/api:trtorch"
					],
					linkstatic = False,  # Change.
					linkshared = True
				)

			apt install git

			bazel build //:libtrtorch --compilation_mode=opt --verbose_failures
			bazel build //:libtrtorch --compilation_mode=dbg
				Output:
					${TRTorch_HOME}/bazel-TRTorch/external/libtorch
					${TRTorch_HOME}/bazel-bin
			bazel clean --async

			export LD_LIBRARY_PATH=${TRTorch_HOME}/bazel-TRTorch/external/libtorch/lib:${TRTorch_HOME}/bazel-bin/_solib_k8:$LD_LIBRARY_PATH

			Compile the Python package.
				cd ${TRTorch_HOME}
				py/build_whl.sh

			Install the Python package.
				pip install ${TRTorch_HOME}/py/dist/trtorch-0.1.0-cp37-cp37m-linux_x86_64.whl

		Using pip:
			https://github.com/NVIDIA/TRTorch/releases/tag/v0.0.3

			(Optional)
				ln -s /usr/lib/x86_64-linux-gnu/libcudnn.so.8 /usr/lib/x86_64-linux-gnu/libcudnn.so.7
					It's a trick.

			pip install https://github.com/NVIDIA/TRTorch/releases/download/v0.0.3/trtorch-0.0.3-cp36-cp36m-linux_x86_64.whl
			pip install https://github.com/NVIDIA/TRTorch/releases/download/v0.0.3/trtorch-0.0.3-cp37-cp37m-linux_x86_64.whl
			pip install https://github.com/NVIDIA/TRTorch/releases/download/v0.0.3/trtorch-0.0.3-cp38-cp38-linux_x86_64.whl

		(Optional) ln -s bazel-TRTorch_github bazel-TRTorch

		Check TRTorch.
			python -c "import trtorch"

		Trouble shooting:
			<error> ImportError: /path/to/TRTorch/bazel-bin/_solib_k8/libcore_Sutil_Slibtrt_Uutil.so: undefined symbol: _ZN3c105ErrorC1ENS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
				<solution>
					There are two ways to solve this problem:
						1) build cpp extensions with -D_GLIBCXX_USE_CXX11_ABI=1.
						2) build pytorch with -D_GLIBCXX_USE_CXX11_ABI=0.
					Edit ${TRTorch_HOME}/py/setup.py
						<before> CXX11_ABI = False
						<after> CXX11_ABI = True

	- Build a docker image.
		https://si-analytics.tistory.com/32

		Edit a Dockerfile.
			./Dockerfile.tensorrt & ./Dockerfile.cuda
			Use Dockerfile.tensorrt.
				Docker image size is smaller.

		Rename Dockerfile.tensorrt to Dockerfile.
		docker build --tag sangwook/tensorrt_trtorch:latest .
			This command "docker build --tag sangwook/tensorrt_trtorch:latest - < Dockerfile.tensorrt" causes an error.

		Rename Dockerfile.cuda to Dockerfile.
		docker build --tag sangwook/cuda_trtorch:latest .
			This command "docker build --tag sangwook/cuda_trtorch:latest - < Dockerfile.cuda" causes an error.

	- Run a docker image.
		docker run --gpus all -it --rm -v ~:/workspace/sangwook --shm-size 16G --name my_tensorrt_trtorch sangwook/tensorrt_trtorch:latest python3 -c "import tensorrt, trtorch"
		NV_GPU=0 nvidia-docker run -it --rm -v ~:/workspace/sangwook --shm-size 16G --name my_tensorrt_trtorch sangwook/tensorrt_trtorch:latest python3 -c "import tensorrt, trtorch"

		docker run --gpus all -it --rm -v ~:/workspace/sangwook --shm-size 16G --name my_cuda_trtorch sangwook/cuda_trtorch:latest python3 -c "import tensorrt, trtorch"
		NV_GPU=0 nvidia-docker run -it --rm -v ~:/workspace/sangwook --shm-size 16G --name my_cuda_trtorch sangwook/cuda_trtorch:latest python3 -c "import tensorrt, trtorch"

[-] Installation (Torch-TensorRT).
	https://pytorch.org/TensorRT/getting_started/installation.html

	- Dependencies.
		Refer to "Installation (TensorRT)".

		PyTorch (Python) or LibTorch (C++).
			https://www.pytorch.org
		CUDA, cuDNN and TensorRT.
			https://developer.nvidia.com/cuda
			https://developer.nvidia.com/cudnn
			https://developer.nvidia.com/tensorrt

	- Python packages.
		pip install nvidia-pyindex
		pip install nvidia-tensorrt
		pip install torch-tensorrt -f https://github.com/NVIDIA/Torch-TensorRT/releases

	- C++ binary distribution.
		https://github.com/pytorch/TensorRT/releases
